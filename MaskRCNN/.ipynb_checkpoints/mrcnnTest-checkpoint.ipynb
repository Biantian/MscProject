{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "Amount of train images:\n",
      "Dataset CocoSubset\n",
      "    Number of datapoints: 19759\n",
      "    Root location: E:/Resource/Dataset/COCO/SubCOCO\\train2017\n",
      "Amount of validation images:\n",
      "Dataset CocoSubset\n",
      "    Number of datapoints: 870\n",
      "    Root location: E:/Resource/Dataset/COCO/SubCOCO\\val2017\n",
      "19759\n",
      "<class 'coco.CocoSubset'>\n",
      "<PIL.Image.Image image mode=RGB size=480x640 at 0x257BB0FDC48> {'labels': tensor([1, 4]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), 'areas': tensor([15185.1797,  3406.1824]), 'boxes': tensor([[ 52.3600, 387.7700, 228.7200, 545.3900],\n",
      "        [ 52.6400, 353.7800, 177.2200, 413.2500]]), 'image_id': tensor([64]), 'iscrowd': tensor([0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "from coco import CocoSubset\n",
    "\n",
    "root = 'E:/Resource/Dataset/COCO/SubCOCO'\n",
    "annDir = os.path.join(root, 'annotations/instances_{}.json')\n",
    "# coco = COCO(annDir.format('train2017'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.68s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "img_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(0.5)\n",
    "    ]),\n",
    "    'val':transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])}\n",
    "\n",
    "target_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5)\n",
    "    ]),\n",
    "    'val':transforms.Compose([])}\n",
    "\n",
    "coco_train = CocoSubset(os.path.join(root, 'train2017'),\n",
    "                        annDir.format('train2017'),\n",
    "                        img_transform=img_transform['train'],\n",
    "                        target_transform=target_transform['train'])\n",
    "\n",
    "coco_val = CocoSubset(os.path.join(root, 'val2017'),\n",
    "                      annDir.format('val2017'),\n",
    "                      img_transform=img_transform['val'],\n",
    "                      target_transform=target_transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19759\n",
      "<class 'coco.CocoSubset'>\n",
      "(tensor([[[0.5216, 0.5176, 0.5412,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.5451, 0.5176, 0.5294,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.5373, 0.5098, 0.5216,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [0.4196, 0.4902, 0.4157,  ..., 0.6549, 0.5686, 0.5490],\n",
      "         [0.4392, 0.4196, 0.4275,  ..., 0.7020, 0.6471, 0.5647],\n",
      "         [0.4784, 0.4549, 0.4078,  ..., 0.6157, 0.6824, 0.6314]],\n",
      "\n",
      "        [[0.4941, 0.5333, 0.5176,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.5098, 0.5216, 0.5059,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.5020, 0.5098, 0.5059,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [0.2902, 0.4235, 0.3569,  ..., 0.6039, 0.5647, 0.5059],\n",
      "         [0.2980, 0.4196, 0.3529,  ..., 0.6510, 0.6392, 0.5098],\n",
      "         [0.3059, 0.4667, 0.3490,  ..., 0.5843, 0.6745, 0.6078]],\n",
      "\n",
      "        [[0.5216, 0.5373, 0.5255,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.5294, 0.5294, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.5216, 0.5176, 0.5098,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [0.2627, 0.3922, 0.2824,  ..., 0.5725, 0.5490, 0.4980],\n",
      "         [0.3059, 0.3725, 0.2941,  ..., 0.6157, 0.5922, 0.4667],\n",
      "         [0.2549, 0.4392, 0.3373,  ..., 0.5333, 0.6235, 0.5451]]]), {'labels': tensor([1, 4]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), 'areas': tensor([15236.,  3392.]), 'boxes': tensor([[ 52., 388., 228., 544.],\n",
      "        [ 53., 354., 176., 412.]]), 'image_id': tensor([64]), 'iscrowd': tensor([0, 0])})\n"
     ]
    }
   ],
   "source": [
    "print(len(coco_train))\n",
    "print(type(coco_train))\n",
    "print(coco_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(coco_train,\n",
    "                                         batch_size=2,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=8,\n",
    "                                         collate_fn=utils.collate_fn)\n",
    "data_loader_val = torch.utils.data.DataLoader(coco_val,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=8,\n",
    "                                              collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x257cc128708>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                      hidden_layer,\n",
    "                                                      num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                            lr=0.005,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                              step_size=3,\n",
    "                                              gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Application\\Anaconda3\\envs\\project_env\\lib\\site-packages\\torch\\nn\\functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "E:\\Application\\Anaconda3\\envs\\project_env\\lib\\site-packages\\torchvision\\ops\\boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  keep = keep.nonzero().squeeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/9880]  eta: 36 days, 18:16:06  lr: 0.000010  loss: 5.0507 (5.0507)  loss_classifier: 1.5123 (1.5123)  loss_box_reg: 0.1240 (0.1240)  loss_mask: 2.8487 (2.8487)  loss_objectness: 0.5347 (0.5347)  loss_rpn_box_reg: 0.0311 (0.0311)  time: 321.4743  data: 129.1690  max mem: 2361\n",
      "Epoch: [0]  [  10/9880]  eta: 3 days, 20:56:20  lr: 0.000060  loss: 4.4112 (4.5674)  loss_classifier: 1.4077 (1.3814)  loss_box_reg: 0.2502 (0.2804)  loss_mask: 2.7185 (2.5674)  loss_objectness: 0.0749 (0.2806)  loss_rpn_box_reg: 0.0311 (0.0575)  time: 33.8988  data: 11.9446  max mem: 3750\n",
      "Epoch: [0]  [  20/9880]  eta: 2 days, 3:42:13  lr: 0.000110  loss: 3.3185 (3.7635)  loss_classifier: 1.1814 (1.1534)  loss_box_reg: 0.1733 (0.2475)  loss_mask: 1.7061 (2.0457)  loss_objectness: 0.0739 (0.2662)  loss_rpn_box_reg: 0.0191 (0.0507)  time: 3.7478  data: 0.1391  max mem: 3750\n",
      "Epoch: [0]  [  30/9880]  eta: 1 day, 12:06:57  lr: 0.000160  loss: 2.1135 (3.1082)  loss_classifier: 0.6262 (0.9084)  loss_box_reg: 0.1535 (0.2290)  loss_mask: 1.3033 (1.7061)  loss_objectness: 0.1166 (0.2231)  loss_rpn_box_reg: 0.0136 (0.0417)  time: 1.8153  data: 0.0636  max mem: 3750\n",
      "Epoch: [0]  [  40/9880]  eta: 1 day, 4:01:22  lr: 0.000210  loss: 1.6804 (2.7381)  loss_classifier: 0.3422 (0.7625)  loss_box_reg: 0.2041 (0.2364)  loss_mask: 0.8307 (1.4915)  loss_objectness: 0.1100 (0.2038)  loss_rpn_box_reg: 0.0149 (0.0439)  time: 1.1957  data: 0.0546  max mem: 3750\n",
      "Epoch: [0]  [  50/9880]  eta: 23:15:25  lr: 0.000260  loss: 1.4104 (2.4748)  loss_classifier: 0.2246 (0.6603)  loss_box_reg: 0.1499 (0.2270)  loss_mask: 0.6892 (1.3268)  loss_objectness: 0.0730 (0.2180)  loss_rpn_box_reg: 0.0243 (0.0427)  time: 1.2596  data: 0.1410  max mem: 3750\n",
      "Epoch: [0]  [  60/9880]  eta: 20:06:11  lr: 0.000310  loss: 1.1792 (2.2737)  loss_classifier: 0.2186 (0.6050)  loss_box_reg: 0.1427 (0.2355)  loss_mask: 0.5422 (1.1928)  loss_objectness: 0.0623 (0.1997)  loss_rpn_box_reg: 0.0245 (0.0407)  time: 1.4606  data: 0.2079  max mem: 3750\n",
      "Epoch: [0]  [  70/9880]  eta: 17:48:41  lr: 0.000360  loss: 1.1792 (2.1355)  loss_classifier: 0.2199 (0.5536)  loss_box_reg: 0.1619 (0.2354)  loss_mask: 0.4949 (1.0968)  loss_objectness: 0.0453 (0.2073)  loss_rpn_box_reg: 0.0183 (0.0425)  time: 1.4848  data: 0.1795  max mem: 3750\n",
      "Epoch: [0]  [  80/9880]  eta: 16:01:16  lr: 0.000410  loss: 0.9689 (1.9949)  loss_classifier: 0.1838 (0.5107)  loss_box_reg: 0.1222 (0.2266)  loss_mask: 0.4325 (1.0226)  loss_objectness: 0.0775 (0.1941)  loss_rpn_box_reg: 0.0174 (0.0408)  time: 1.3578  data: 0.0957  max mem: 3750\n",
      "Epoch: [0]  [  90/9880]  eta: 14:38:51  lr: 0.000460  loss: 0.9934 (1.8978)  loss_classifier: 0.1948 (0.4826)  loss_box_reg: 0.2243 (0.2317)  loss_mask: 0.4288 (0.9561)  loss_objectness: 0.0414 (0.1865)  loss_rpn_box_reg: 0.0174 (0.0409)  time: 1.3034  data: 0.0241  max mem: 3750\n",
      "Epoch: [0]  [ 100/9880]  eta: 13:33:57  lr: 0.000509  loss: 1.1164 (1.8301)  loss_classifier: 0.1948 (0.4537)  loss_box_reg: 0.1648 (0.2266)  loss_mask: 0.4170 (0.9209)  loss_objectness: 0.0413 (0.1881)  loss_rpn_box_reg: 0.0166 (0.0408)  time: 1.3819  data: 0.0682  max mem: 3750\n",
      "Epoch: [0]  [ 110/9880]  eta: 12:37:52  lr: 0.000559  loss: 1.1587 (1.7784)  loss_classifier: 0.2000 (0.4366)  loss_box_reg: 0.1648 (0.2313)  loss_mask: 0.4564 (0.8776)  loss_objectness: 0.1332 (0.1888)  loss_rpn_box_reg: 0.0241 (0.0441)  time: 1.3236  data: 0.0542  max mem: 3750\n",
      "Epoch: [0]  [ 120/9880]  eta: 11:51:38  lr: 0.000609  loss: 1.2885 (1.7317)  loss_classifier: 0.2033 (0.4180)  loss_box_reg: 0.2550 (0.2309)  loss_mask: 0.4639 (0.8544)  loss_objectness: 0.1059 (0.1832)  loss_rpn_box_reg: 0.0299 (0.0453)  time: 1.2501  data: 0.0196  max mem: 3750\n",
      "Epoch: [0]  [ 130/9880]  eta: 11:14:04  lr: 0.000659  loss: 1.1919 (1.6832)  loss_classifier: 0.1846 (0.4012)  loss_box_reg: 0.1870 (0.2293)  loss_mask: 0.5279 (0.8289)  loss_objectness: 0.0970 (0.1756)  loss_rpn_box_reg: 0.0345 (0.0481)  time: 1.3392  data: 0.0140  max mem: 3750\n",
      "Epoch: [0]  [ 140/9880]  eta: 10:41:47  lr: 0.000709  loss: 0.9602 (1.6275)  loss_classifier: 0.1400 (0.3845)  loss_box_reg: 0.1658 (0.2325)  loss_mask: 0.4011 (0.7985)  loss_objectness: 0.0530 (0.1661)  loss_rpn_box_reg: 0.0119 (0.0458)  time: 1.4047  data: 0.0090  max mem: 3750\n",
      "Epoch: [0]  [ 150/9880]  eta: 10:12:48  lr: 0.000759  loss: 0.9343 (1.5817)  loss_classifier: 0.1766 (0.3709)  loss_box_reg: 0.1889 (0.2310)  loss_mask: 0.3558 (0.7722)  loss_objectness: 0.0530 (0.1633)  loss_rpn_box_reg: 0.0115 (0.0443)  time: 1.3600  data: 0.0232  max mem: 3750\n",
      "Epoch: [0]  [ 160/9880]  eta: 9:47:39  lr: 0.000809  loss: 0.9343 (1.5423)  loss_classifier: 0.1766 (0.3573)  loss_box_reg: 0.1743 (0.2273)  loss_mask: 0.3618 (0.7508)  loss_objectness: 0.0723 (0.1629)  loss_rpn_box_reg: 0.0143 (0.0440)  time: 1.3292  data: 0.0257  max mem: 3750\n",
      "Epoch: [0]  [ 170/9880]  eta: 9:24:09  lr: 0.000859  loss: 0.8448 (1.5126)  loss_classifier: 0.1234 (0.3458)  loss_box_reg: 0.1176 (0.2218)  loss_mask: 0.4673 (0.7362)  loss_objectness: 0.0931 (0.1651)  loss_rpn_box_reg: 0.0231 (0.0438)  time: 1.2752  data: 0.0145  max mem: 3750\n",
      "Epoch: [0]  [ 180/9880]  eta: 9:02:40  lr: 0.000909  loss: 0.9444 (1.4884)  loss_classifier: 0.1383 (0.3362)  loss_box_reg: 0.1178 (0.2197)  loss_mask: 0.4997 (0.7252)  loss_objectness: 0.1243 (0.1626)  loss_rpn_box_reg: 0.0251 (0.0446)  time: 1.1772  data: 0.0062  max mem: 3750\n",
      "Epoch: [0]  [ 190/9880]  eta: 8:44:06  lr: 0.000959  loss: 0.9444 (1.4541)  loss_classifier: 0.1203 (0.3262)  loss_box_reg: 0.1182 (0.2166)  loss_mask: 0.4534 (0.7070)  loss_objectness: 0.0897 (0.1595)  loss_rpn_box_reg: 0.0259 (0.0448)  time: 1.1863  data: 0.0600  max mem: 3750\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All bounding boxes should have positive height and width. Found invaid box [801.5234375, 328.3018798828125, 801.5234375, 328.3018798828125] for target at index 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e3ea1c14a45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Study\\Program\\Python\\MscProject\\MaskRCNN\\engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Application\\Anaconda3\\envs\\project_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Application\\Anaconda3\\envs\\project_env\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     91\u001b[0m                     raise ValueError(\"All bounding boxes should have positive height and width.\"\n\u001b[0;32m     92\u001b[0m                                      \u001b[1;34m\" Found invaid box {} for target at index {}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                                      .format(degen_bb, target_idx))\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All bounding boxes should have positive height and width. Found invaid box [801.5234375, 328.3018798828125, 801.5234375, 328.3018798828125] for target at index 1."
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_val, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
